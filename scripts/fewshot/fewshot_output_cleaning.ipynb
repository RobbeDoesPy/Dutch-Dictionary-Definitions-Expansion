{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e1b46d-789f-46fd-8b5d-a37e140ad829",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing file: evaluation_results_per_entry_Geitje_few_shot.tsv ---\n",
      "Removed metric columns: bertscore_precision, bertscore_recall, bertscore_f1, comet22_score, cometkiwi_score, xcomet_score\n",
      "Created 'model_long_definition' column.\n",
      "  - Adapted for 'Lange definitie: ': 684 times.\n",
      "  - Predictions left unchanged: 2766 times.\n",
      "Successfully saved processed file to: processed_results/processed_evaluation_results_per_entry_Geitje_few_shot.tsv\n",
      "\n",
      "--- Processing file: evaluation_results_per_entry_aya-23_few_shot.tsv ---\n",
      "Removed metric columns: bertscore_precision, bertscore_recall, bertscore_f1, comet22_score, cometkiwi_score, xcomet_score\n",
      "Created 'model_long_definition' column.\n",
      "  - Adapted for '**Volledige definitie:** ': 1238 times.\n",
      "  - Adapted for '**Lange definitie:** ': 1966 times.\n",
      "  - Predictions left unchanged: 246 times.\n",
      "Successfully saved processed file to: processed_results/processed_evaluation_results_per_entry_aya-23_few_shot.tsv\n",
      "\n",
      "--- Processing file: evaluation_results_per_entry_aya-101_few_shot.tsv ---\n",
      "No metric columns found to remove.\n",
      "Created 'model_long_definition' column.\n",
      "  - Adapted for 'Lange definitie: ': 1649 times.\n",
      "  - Predictions left unchanged: 1801 times.\n",
      "Successfully saved processed file to: processed_results/processed_evaluation_results_per_entry_aya-101_few_shot.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# Cell 1: Pre-processing and Cleaning\n",
    "# ==============================================================================\n",
    "\n",
    "def process_evaluation_files(file_paths, output_dir=\"processed_results\"):\n",
    "    \"\"\"\n",
    "    Processes evaluation files to remove metric columns and extract long definitions.\n",
    "\n",
    "    Args:\n",
    "        file_paths (list): A list of paths to the input TSV files.\n",
    "        output_dir (str): The directory to save the processed files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "    # Columns to be removed if they exist\n",
    "    metric_columns_to_drop = [\n",
    "        'bertscore_precision', 'bertscore_recall', 'bertscore_f1',\n",
    "        'comet22_score', 'cometkiwi_score', 'xcomet_score'\n",
    "    ]\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            print(f\"\\n--- Processing file: {os.path.basename(file_path)} ---\")\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # 1. Eliminate metric columns\n",
    "            existing_metrics = [col for col in metric_columns_to_drop if col in df.columns]\n",
    "            if existing_metrics:\n",
    "                df = df.drop(columns=existing_metrics)\n",
    "                print(f\"Removed metric columns: {', '.join(existing_metrics)}\")\n",
    "            else:\n",
    "                print(\"No metric columns found to remove.\")\n",
    "\n",
    "            # 2. Extract long definitions\n",
    "            if 'model_prediction' in df.columns:\n",
    "                counters = {\n",
    "                    \"geitje_adapted\": 0,\n",
    "                    \"aya_101_adapted\": 0,\n",
    "                    \"aya_volledige_adapted\": 0,\n",
    "                    \"aya_lange_adapted\": 0,\n",
    "                    \"not_adapted\": 0,\n",
    "                }\n",
    "\n",
    "                def extract_long_definition(prediction, filename):\n",
    "                    \"\"\"Extracts the long definition based on file-specific markers.\"\"\"\n",
    "                    prediction_str = str(prediction)\n",
    "                    \n",
    "                    if \"aya-101\" in filename or \"Geitje\" in filename:\n",
    "                        marker = \"Lange definitie: \"\n",
    "                        if marker in prediction_str:\n",
    "                            if \"aya-101\" in filename:\n",
    "                                counters[\"aya_101_adapted\"] += 1\n",
    "                            else: # Geitje\n",
    "                                counters[\"geitje_adapted\"] += 1\n",
    "                            return prediction_str.split(marker, 1)[-1].strip()\n",
    "                    elif \"aya-23\" in filename:\n",
    "                        marker_volledige = \"**Volledige definitie:** \"\n",
    "                        marker_lange = \"**Lange definitie:** \"\n",
    "                        if marker_volledige in prediction_str:\n",
    "                            counters[\"aya_volledige_adapted\"] += 1\n",
    "                            return prediction_str.split(marker_volledige, 1)[-1].strip()\n",
    "                        elif marker_lange in prediction_str:\n",
    "                            counters[\"aya_lange_adapted\"] += 1\n",
    "                            return prediction_str.split(marker_lange, 1)[-1].strip()\n",
    "\n",
    "                    counters[\"not_adapted\"] += 1\n",
    "                    return prediction_str\n",
    "\n",
    "                df['model_long_definition'] = df['model_prediction'].apply(\n",
    "                    lambda x: extract_long_definition(x, os.path.basename(file_path))\n",
    "                )\n",
    "                print(\"Created 'model_long_definition' column.\")\n",
    "\n",
    "                # Print the counts for the current file\n",
    "                if \"Geitje\" in os.path.basename(file_path):\n",
    "                    print(f\"  - Adapted for 'Lange definitie: ': {counters['geitje_adapted']} times.\")\n",
    "                if \"aya-101\" in os.path.basename(file_path):\n",
    "                    print(f\"  - Adapted for 'Lange definitie: ': {counters['aya_101_adapted']} times.\")\n",
    "                if \"aya-23\" in os.path.basename(file_path):\n",
    "                    print(f\"  - Adapted for '**Volledige definitie:** ': {counters['aya_volledige_adapted']} times.\")\n",
    "                    print(f\"  - Adapted for '**Lange definitie:** ': {counters['aya_lange_adapted']} times.\")\n",
    "                print(f\"  - Predictions left unchanged: {counters['not_adapted']} times.\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"Column 'model_prediction' not found.\")\n",
    "\n",
    "            # Save the processed dataframe\n",
    "            output_filename = f\"processed_{os.path.basename(file_path)}\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            df.to_csv(output_path, sep='\\t', index=False)\n",
    "            print(f\"Successfully saved processed file to: {output_path}\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file was not found at {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # List of your input files\n",
    "    # Add any other files you want to process to this list\n",
    "    files_to_process = [\n",
    "        \"evaluation_results_per_entry_Geitje_few_shot.tsv\",\n",
    "        \"evaluation_results_per_entry_aya-23_few_shot.tsv\",\n",
    "        \"evaluation_results_per_entry_aya-101_few_shot.tsv\",\n",
    "        # \"path/to/your/file_without_metrics.tsv\" # Example for the file without metrics\n",
    "    ]\n",
    "    process_evaluation_files(files_to_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196b288-747f-429f-b747-d8f966ebec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# ==============================================================================\n",
    "# Cell 2: Metric Calculation on Processed Files\n",
    "# ==============================================================================\n",
    "\n",
    "def calculate_metrics_on_processed(processed_dir=\"processed_results\", metrics_output_dir=\"metrics_results\"):\n",
    "    \"\"\"\n",
    "    Calculates metrics on the processed evaluation files.\n",
    "\n",
    "    Args:\n",
    "        processed_dir (str): Directory containing the processed TSV files.\n",
    "        metrics_output_dir (str): Directory to save the metrics results.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(metrics_output_dir):\n",
    "        os.makedirs(metrics_output_dir)\n",
    "        print(f\"\\nCreated metrics output directory: {metrics_output_dir}\")\n",
    "\n",
    "    processed_files = [f for f in os.listdir(processed_dir) if f.startswith(\"processed_\") and f.endswith(\".tsv\")]\n",
    "\n",
    "    if not processed_files:\n",
    "        print(f\"No processed files found in '{processed_dir}'. Please run the first cell.\")\n",
    "        return\n",
    "\n",
    "    # Load all metric models once\n",
    "    print(\"\\nLoading evaluation models...\")\n",
    "    rouge = evaluate.load('rouge')\n",
    "    bleu = evaluate.load('bleu')\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    comet_22 = evaluate.load('comet', 'Unbabel/wmt22-comet-da')\n",
    "    comet_kiwi = evaluate.load('comet', 'Unbabel/wmt22-cometkiwi-da')\n",
    "    xcomet = evaluate.load('comet', 'Unbabel/XCOMET-XL')\n",
    "    print(\"Evaluation models loaded.\")\n",
    "\n",
    "    for filename in processed_files:\n",
    "        try:\n",
    "            file_path = os.path.join(processed_dir, filename)\n",
    "            print(f\"\\n--- Calculating metrics for: {filename} ---\")\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "            # Define predictions, references, and sources\n",
    "            predictions = df[\"model_long_definition\"].tolist()\n",
    "            references = df[\"DefinitionFull\"].tolist()\n",
    "            sources = df[\"DefinitionShort\"].tolist() # Using short definition as source for COMET\n",
    "\n",
    "            # --- ROUGE ---\n",
    "            rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "            print(\"\\n--- ROUGE Scores ---\")\n",
    "            print(rouge_results)\n",
    "\n",
    "            # --- BLEU ---\n",
    "            bleu_results = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "            print(\"\\n--- BLEU Score ---\")\n",
    "            print(bleu_results)\n",
    "\n",
    "            # --- BERTScore ---\n",
    "            bertscore_results = bertscore.compute(predictions=predictions, references=references, lang=\"nl\")\n",
    "            avg_precision = sum(bertscore_results['precision']) / len(bertscore_results['precision'])\n",
    "            avg_recall = sum(bertscore_results['recall']) / len(bertscore_results['recall'])\n",
    "            avg_f1 = sum(bertscore_results['f1']) / len(bertscore_results['f1'])\n",
    "            print(f\"\\n--- BERTScore ---\")\n",
    "            print(f\"{'Average Precision':>20}: {avg_precision:.4f}\")\n",
    "            print(f\"{'Average Recall':>20}: {avg_recall:.4f}\")\n",
    "            print(f\"{'Average F1':>20}: {avg_f1:.4f}\")\n",
    "\n",
    "            # --- COMET ---\n",
    "            print(\"\\n--- COMET Scores ---\")\n",
    "            comet_22_results = comet_22.compute(predictions=predictions, references=references, sources=sources)\n",
    "            comet_kiwi_results = comet_kiwi.compute(predictions=predictions, references=references, sources=sources)\n",
    "            xcomet_results = xcomet.compute(predictions=predictions, references=references, sources=sources)\n",
    "            print(f\"{'COMET-22':>20}: {comet_22_results['mean_score']:.4f}\")\n",
    "            print(f\"{'COMETkiwi':>20}: {comet_kiwi_results['mean_score']:.4f}\")\n",
    "            print(f\"{'XCOMET':>20}: {xcomet_results['mean_score']:.4f}\")\n",
    "\n",
    "            # --- Save Results ---\n",
    "            # Extract model name and shot type from filename for saving\n",
    "            parts = filename.replace(\"processed_evaluation_results_per_entry_\", \"\").replace(\".tsv\", \"\").split(\"_\")\n",
    "            short_model_name = parts[0]\n",
    "            shot_type = \"_\".join(parts[1:])\n",
    "\n",
    "            # Save summary\n",
    "            summary_path = os.path.join(metrics_output_dir, f\"metrics_summary_{short_model_name}_{shot_type}.txt\")\n",
    "            with open(summary_path, \"w\") as f:\n",
    "                f.write(f\"Metrics Summary for {short_model_name} ({shot_type})\\n\\n\")\n",
    "                f.write(\"--- ROUGE Scores ---\\n\" + str(rouge_results) + \"\\n\")\n",
    "                f.write(\"\\n--- BLEU Score ---\\n\" + str(bleu_results) + \"\\n\")\n",
    "                f.write(\"\\n--- BERTScore ---\\n\")\n",
    "                f.write(f\"{'Average Precision':>20}: {avg_precision:.4f}\\n\")\n",
    "                f.write(f\"{'Average Recall':>20}: {avg_recall:.4f}\\n\")\n",
    "                f.write(f\"{'Average F1':>20}: {avg_f1:.4f}\\n\")\n",
    "                f.write(\"\\n--- COMET Scores ---\\n\")\n",
    "                f.write(f\"{'COMET-22':>20}: {comet_22_results['mean_score']:.4f}\\n\")\n",
    "                f.write(f\"{'COMETkiwi':>20}: {comet_kiwi_results['mean_score']:.4f}\\n\")\n",
    "                f.write(f\"{'XCOMET':>20}: {xcomet_results['mean_score']:.4f}\\n\")\n",
    "            print(f\"\\nSummary of scores saved to: {summary_path}\")\n",
    "\n",
    "            # Save detailed per-entry TSV\n",
    "            df['bertscore_precision'] = bertscore_results['precision']\n",
    "            df['bertscore_recall'] = bertscore_results['recall']\n",
    "            df['bertscore_f1'] = bertscore_results['f1']\n",
    "            df['comet22_score'] = comet_22_results['scores']\n",
    "            df['cometkiwi_score'] = comet_kiwi_results['scores']\n",
    "            df['xcomet_score'] = xcomet_results['scores']\n",
    "            detailed_path = os.path.join(metrics_output_dir, f\"metrics_detailed_{short_model_name}_{shot_type}.tsv\")\n",
    "            df.to_csv(detailed_path, sep='\\t', index=False)\n",
    "            print(f\"Detailed metrics saved to: {detailed_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while calculating metrics for {filename}: {e}\")\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # --- Cell 2 Execution ---\n",
    "    calculate_metrics_on_processed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
