{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb454b7-9433-455b-bd33-12f35d071b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caeeb927380b4f02982710344a69825c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a3499528dd498d9a3fdc5168d849ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b953b0e7b754282a730e41e06960c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/381k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2502ba103e48e0aba2e21ab0d9bda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/379k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e0b69337a54594ab7f42973a3af7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/27880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca75d64f53b4e52a5374f5fe0013836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3494 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4f7039be18479caddfbba1207d9cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded few-shot examples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, BitsAndBytesConfig, logging\n",
    "import evaluate\n",
    "\n",
    "# --- Basic Setup ---\n",
    "# Suppress verbose logging for cleaner output\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Define the models to be evaluated\n",
    "MODEL_IDS = [\n",
    "    \"google/mt5-xl\"\n",
    "]\n",
    "\n",
    "# Load the shared dataset\n",
    "dataset = load_dataset(\"RobbedoesHF/dutch-definitions\", split=\"test\")\n",
    "\n",
    "# Load the few-shot examples from the JSONL file\n",
    "try:\n",
    "    with open('prompts_alpha_0_8.jsonl', 'r', encoding='utf-8') as f:\n",
    "        few_shot_examples = [json.loads(line) for line in f]\n",
    "    # Create a dictionary and a single example for prompts\n",
    "    few_shot_dict = {example['lemma']: example for example in few_shot_examples}\n",
    "    one_shot_example = few_shot_examples[0]['few_shot_example_1']\n",
    "    print(\"Successfully loaded few-shot examples.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'prompts_alpha_0_8.jsonl' not found. Few-shot and one-shot evaluations will not work.\")\n",
    "    few_shot_dict = {}\n",
    "    one_shot_example = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913b4940-874a-4f6e-a057-95a8d63d1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt Creation Function ---\n",
    "def create_prompt(model_id, tokenizer, shot_type, lemma, gloss, one_shot_example=None, few_shot_dict=None):\n",
    "    \"\"\"Creates a structured prompt based on the model, shot type, and input.\"\"\"\n",
    "    \n",
    "    system_prompt = \"Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.\"\n",
    "    base_instruction = f\"Breid de volgende korte definitie voor het woord '{lemma}' uit tot een volledige definitie: '{gloss}'\"\n",
    "\n",
    "    # --- Prompts for Aya models ---\n",
    "    if model_id in [\"CohereLabs/aya-101\", \"google/mt5-xl\"]:\n",
    "        if shot_type == 'zero_shot' and model_id == \"CohereLabs/aya-101\":\n",
    "            return f\"{system_prompt}\\n\\n{base_instruction}\"\n",
    "\n",
    "        elif shot_type == 'zero_shot' and model_id == \"google/mt5-xl\":\n",
    "            return base_instruction\n",
    "        \n",
    "        elif shot_type == 'one_shot':\n",
    "            one_shot_text = f\"Voorbeeld:\\nWoord: {one_shot_example['lemma']}\\nKorte definitie: {one_shot_example['short_definition']}\\nLange definitie: {one_shot_example['long_definition']}\"\n",
    "            return f\"{system_prompt}\\n\\n{one_shot_text}\\n\\n{base_instruction}\"\n",
    "\n",
    "        elif shot_type == 'few_shot':\n",
    "            examples = few_shot_dict.get(lemma, {})\n",
    "            few_shot_text = \"Voorbeelden:\\n\"\n",
    "            for i in range(1, 4):\n",
    "                example = examples.get(f'few_shot_example_{i}')\n",
    "                if example:\n",
    "                    few_shot_text += f\"Woord: {example['lemma']}\\nKorte definitie: {example['short_definition']}\\nLange definitie: {example['long_definition']}\\n\\n\"\n",
    "            return f\"{system_prompt}\\n\\n{few_shot_text}{base_instruction}\"\n",
    "\n",
    "    # --- Prompts for Aya-23 (with Chat Template) ---\n",
    "    elif model_id == \"CohereLabs/aya-expanse-8b\":\n",
    "        # For this model, the entire user-facing prompt is constructed first,\n",
    "        # then wrap it in the required chat tokens as a single user turn.\n",
    "        user_content = \"\"\n",
    "        if shot_type == 'zero_shot':\n",
    "            user_content = f\"{system_prompt}\\n\\n{base_instruction}\"\n",
    "        \n",
    "        elif shot_type == 'one_shot':\n",
    "            one_shot_text = f\"Voorbeeld:\\nWoord: {one_shot_example['lemma']}\\nKorte definitie: {one_shot_example['short_definition']}\\nLange definitie: {one_shot_example['long_definition']}\"\n",
    "            user_content = f\"{system_prompt}\\n\\n{one_shot_text}\\n\\n{base_instruction}\"\n",
    "\n",
    "        elif shot_type == 'few_shot':\n",
    "            examples = few_shot_dict.get(lemma, {})\n",
    "            few_shot_text = \"Voorbeelden:\\n\"\n",
    "            for i in range(1, 4):\n",
    "                example = examples.get(f'few_shot_example_{i}')\n",
    "                if example:\n",
    "                    few_shot_text += f\"Woord: {example['lemma']}\\nKorte definitie: {example['short_definition']}\\nLange definitie: {example['long_definition']}\\n\\n\"\n",
    "            user_content = f\"{system_prompt}\\n\\n{few_shot_text}{base_instruction}\"\n",
    "        \n",
    "        # Wrap the complete user content in the model's chat format\n",
    "        return f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{user_content}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n",
    "            \n",
    "    # --- Prompts for Geitje (using single user turn with examples) ---\n",
    "    elif model_id == \"bramvanroy/geitje-7b-ultra\":\n",
    "        # In this approach, we combine all examples and the final instruction\n",
    "        # into a single block of text, which is then given the 'user' role.\n",
    "        user_content = \"\"\n",
    "        if shot_type == 'zero_shot':\n",
    "            user_content = base_instruction\n",
    "        \n",
    "        elif shot_type == 'one_shot':\n",
    "            one_shot_text = f\"Voorbeeld:\\nWoord: {one_shot_example['lemma']}\\nKorte definitie: {one_shot_example['short_definition']}\\nLange definitie: {one_shot_example['long_definition']}\"\n",
    "            user_content = f\"{one_shot_text}\\n\\n{base_instruction}\"\n",
    "\n",
    "        elif shot_type == 'few_shot':\n",
    "            few_shot_text = \"Voorbeelden:\\n\"\n",
    "            examples = few_shot_dict.get(lemma, {})\n",
    "            for i in range(1, 4):\n",
    "                example = examples.get(f'few_shot_example_{i}')\n",
    "                if example:\n",
    "                    few_shot_text += f\"Woord: {example['lemma']}\\nKorte definitie: {example['short_definition']}\\nLange definitie: {example['long_definition']}\\n\\n\"\n",
    "            user_content = f\"{few_shot_text}{base_instruction}\"\n",
    "\n",
    "        # The chat structure now has a system message and one user message containing everything.\n",
    "        chat = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "        \n",
    "        # Apply the template. add_generation_prompt=True will add the final prompt for the assistant to start generating.\n",
    "        return tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54299eab-780a-41f6-a59f-7ea9c3d4946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real data for lemma: '1,5 metermaatschappij'\n",
      "\n",
      "================================================================================\n",
      "PROMPT EXAMPLES FOR MODEL: bramvanroy/geitje-7b-ultra\n",
      "================================================================================\n",
      "\n",
      "--- ZERO_SHOT PROMPT ---\n",
      "<|system|>\n",
      "Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.</s>\n",
      "<|user|>\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'</s>\n",
      "<|assistant|>\n",
      "\n",
      "--- END ZERO_SHOT PROMPT ---\n",
      "\n",
      "--- ONE_SHOT PROMPT ---\n",
      "<|system|>\n",
      "Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.</s>\n",
      "<|user|>\n",
      "Voorbeeld:\n",
      "Woord: anderhalvemetermaatschappij\n",
      "Korte definitie: maatschappij waarin fysieke afstand nodig is\n",
      "Lange definitie: maatschappij waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'</s>\n",
      "<|assistant|>\n",
      "\n",
      "--- END ONE_SHOT PROMPT ---\n",
      "\n",
      "--- FEW_SHOT PROMPT ---\n",
      "<|system|>\n",
      "Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.</s>\n",
      "<|user|>\n",
      "Voorbeelden:\n",
      "Woord: anderhalvemetermaatschappij\n",
      "Korte definitie: maatschappij waarin fysieke afstand nodig is\n",
      "Lange definitie: maatschappij waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Woord: 1,5 metersamenleving\n",
      "Korte definitie: samenleving waarin fysieke afstand nodig is\n",
      "Lange definitie: samenleving waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Woord: anderhalvemetersamenleving\n",
      "Korte definitie: samenleving waarin fysieke afstand nodig is\n",
      "Lange definitie: samenleving waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'</s>\n",
      "<|assistant|>\n",
      "\n",
      "--- END FEW_SHOT PROMPT ---\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PROMPT EXAMPLES FOR MODEL: google/mt5-xl\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ZERO_SHOT PROMPT ---\n",
      "Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'\n",
      "--- END ZERO_SHOT PROMPT ---\n",
      "\n",
      "--- ONE_SHOT PROMPT ---\n",
      "Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.\n",
      "\n",
      "Voorbeeld:\n",
      "Woord: anderhalvemetermaatschappij\n",
      "Korte definitie: maatschappij waarin fysieke afstand nodig is\n",
      "Lange definitie: maatschappij waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'\n",
      "--- END ONE_SHOT PROMPT ---\n",
      "\n",
      "--- FEW_SHOT PROMPT ---\n",
      "Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.\n",
      "\n",
      "Voorbeelden:\n",
      "Woord: anderhalvemetermaatschappij\n",
      "Korte definitie: maatschappij waarin fysieke afstand nodig is\n",
      "Lange definitie: maatschappij waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Woord: 1,5 metersamenleving\n",
      "Korte definitie: samenleving waarin fysieke afstand nodig is\n",
      "Lange definitie: samenleving waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Woord: anderhalvemetersamenleving\n",
      "Korte definitie: samenleving waarin fysieke afstand nodig is\n",
      "Lange definitie: samenleving waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'\n",
      "--- END FEW_SHOT PROMPT ---\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PROMPT EXAMPLES FOR MODEL: CohereLabs/aya-23-8B\n",
      "================================================================================\n",
      "\n",
      "--- ZERO_SHOT PROMPT ---\n",
      "<|START_OF_TURN_TOKEN|><|USER_TOKEN|>Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "--- END ZERO_SHOT PROMPT ---\n",
      "\n",
      "--- ONE_SHOT PROMPT ---\n",
      "<|START_OF_TURN_TOKEN|><|USER_TOKEN|>Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.\n",
      "\n",
      "Voorbeeld:\n",
      "Woord: anderhalvemetermaatschappij\n",
      "Korte definitie: maatschappij waarin fysieke afstand nodig is\n",
      "Lange definitie: maatschappij waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "--- END ONE_SHOT PROMPT ---\n",
      "\n",
      "--- FEW_SHOT PROMPT ---\n",
      "<|START_OF_TURN_TOKEN|><|USER_TOKEN|>Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.\n",
      "\n",
      "Voorbeelden:\n",
      "Woord: anderhalvemetermaatschappij\n",
      "Korte definitie: maatschappij waarin fysieke afstand nodig is\n",
      "Lange definitie: maatschappij waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Woord: 1,5 metersamenleving\n",
      "Korte definitie: samenleving waarin fysieke afstand nodig is\n",
      "Lange definitie: samenleving waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Woord: anderhalvemetersamenleving\n",
      "Korte definitie: samenleving waarin fysieke afstand nodig is\n",
      "Lange definitie: samenleving waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "--- END FEW_SHOT PROMPT ---\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PROMPT EXAMPLES FOR MODEL: CohereLabs/aya-101\n",
      "================================================================================\n",
      "\n",
      "--- ZERO_SHOT PROMPT ---\n",
      "Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'\n",
      "--- END ZERO_SHOT PROMPT ---\n",
      "\n",
      "--- ONE_SHOT PROMPT ---\n",
      "Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.\n",
      "\n",
      "Voorbeeld:\n",
      "Woord: anderhalvemetermaatschappij\n",
      "Korte definitie: maatschappij waarin fysieke afstand nodig is\n",
      "Lange definitie: maatschappij waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'\n",
      "--- END ONE_SHOT PROMPT ---\n",
      "\n",
      "--- FEW_SHOT PROMPT ---\n",
      "Je bent een expert-lexicograaf die definities schrijft voor een Nederlands woordenboek.\n",
      "\n",
      "Voorbeelden:\n",
      "Woord: anderhalvemetermaatschappij\n",
      "Korte definitie: maatschappij waarin fysieke afstand nodig is\n",
      "Lange definitie: maatschappij waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Woord: 1,5 metersamenleving\n",
      "Korte definitie: samenleving waarin fysieke afstand nodig is\n",
      "Lange definitie: samenleving waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Woord: anderhalvemetersamenleving\n",
      "Korte definitie: samenleving waarin fysieke afstand nodig is\n",
      "Lange definitie: samenleving waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "Breid de volgende korte definitie voor het woord '1,5 metermaatschappij' uit tot een volledige definitie: 'maatschappij waarin fysieke afstand nodig is'\n",
      "--- END FEW_SHOT PROMPT ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the actual dataset\n",
    "try:\n",
    "    dataset = load_dataset(\"RobbedoesHF/dutch-definitions\", split=\"test\")\n",
    "    # Take the first entry from the dataset as our real example\n",
    "    real_entry = dataset[0]\n",
    "    sample_lemma = real_entry['Lemma']\n",
    "    sample_gloss = real_entry['DefinitionShort']\n",
    "    print(f\"Using real data for lemma: '{sample_lemma}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load dataset. Using fallback sample data. Error: {e}\")\n",
    "    sample_lemma = \"evaluatie\"\n",
    "    sample_gloss = \"het proces van beoordelen\"\n",
    "\n",
    "# Load the few-shot examples and find the ones corresponding to our real entry\n",
    "try:\n",
    "    with open('prompts_alpha_0_8.jsonl', 'r', encoding='utf-8') as f:\n",
    "        all_few_shot_examples = [json.loads(line) for line in f]\n",
    "    \n",
    "    few_shot_dict_full = {example['lemma']: example for example in all_few_shot_examples}\n",
    "    \n",
    "    # Get the specific few-shot examples for our chosen lemma\n",
    "    sample_few_shot_dict = {sample_lemma: few_shot_dict_full.get(sample_lemma, {})}\n",
    "    \n",
    "    # Use the first of these examples for the one-shot prompt\n",
    "    sample_one_shot_example = sample_few_shot_dict[sample_lemma].get('few_shot_example_1', {})\n",
    "    \n",
    "    if not sample_one_shot_example:\n",
    "        print(\"Warning: Could not find specific few-shot examples for the first dataset entry. Prompts will fall back to zero-shot.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not load few-shot file. Using empty examples. Error: {e}\")\n",
    "    sample_one_shot_example = {}\n",
    "    sample_few_shot_dict = {}\n",
    "\n",
    "\n",
    "# --- Generate and Print Examples ---\n",
    "for model_id in MODEL_IDS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROMPT EXAMPLES FOR MODEL: {model_id}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # We need to load the tokenizer for each model, as it's required for Geitje's prompt creation\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load tokenizer for {model_id}. Skipping. Error: {e}\")\n",
    "        continue\n",
    "\n",
    "    for shot_type in [\"zero_shot\", \"one_shot\", \"few_shot\"]:\n",
    "        prompt = create_prompt(\n",
    "            model_id=model_id,\n",
    "            tokenizer=tokenizer,\n",
    "            shot_type=shot_type,\n",
    "            lemma=sample_lemma,\n",
    "            gloss=sample_gloss,\n",
    "            one_shot_example=sample_one_shot_example,\n",
    "            few_shot_dict=sample_few_shot_dict\n",
    "        )\n",
    "        \n",
    "        print(f\"--- {shot_type.upper()} PROMPT ---\")\n",
    "        print(prompt)\n",
    "        print(f\"--- END {shot_type.upper()} PROMPT ---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da29c71e-ae46-4658-8ffd-6e03fd4c01c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "EVALUATING MODEL: google/mt5-xl\n",
      "====================\n",
      "Loading google/mt5-xl...\n",
      "Model and tokenizer loaded successfully.\n",
      "\n",
      "Generating predictions for 3450 samples using google/mt5-xl (zero_shot)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44e019dcd6149f5bc1faf5a3c190040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- First Batch Generation Results ---\n",
      "\n",
      "--- Sample 1 ---\n",
      "SOURCE: maatschappij waarin fysieke afstand nodig is\n",
      "PREDICTION: <extra_id_0>'. '1,5 metermaatschappij' '1,5 metermaatschappij' '1,5 metermaatschappij' '1,5 metermaatschappij' '1,5 metermaatschappij' '1,5 metermaatschappij' '1,5 metermaatschappij' ' <extra_id_8>' ' <extra_id_9>' ' <extra_id_10>' ' <extra_id_11>' ' <extra_id_12>' ' <extra_id_13>' ' <extra_id_14>' ' <extra_id_15>' ' <extra_id_16>' ' <extra_id_17>' ' <extra_id_18>'  <extra_id_19>' ' <extra_id_25>' ' <extra_id_26>' ' <extra_id_31>' ' <extra_id_32>' ' <extra_id_19>' ' <extra_id_17>' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "REFERENCE: maatschappij waarin mensen die niet tot hetzelfde huishouden behoren in de publieke ruimte een fysieke afstand van minimaal anderhalve meter tot elkaar moeten bewaren om verspreiding van een virus te voorkomen\n",
      "\n",
      "--- Sample 2 ---\n",
      "SOURCE: alarmcentrale\n",
      "PREDICTION: <extra_id_0> 'alarmcentrale' 'alarmcentrale' 'alarmcentrale' 'alarmcentrale' '112-centrale'  <extra_id_1> '112-centrale' '112-centrale' '112-centrale'  <extra_id_8>  <extra_id_6>  <extra_id_7>  <extra_id_8>  <extra_id_6>  <extra_id_7>  <extra_id_8>  <extra_id_7>  <extra_id_16>  <extra_id_17>  <extra_id_18>  <extra_id_19>  <extra_id_20>  <extra_id_21>  <extra_id_22> 'alarmcentrale' '112-centrale' '112-centrale'\n",
      "REFERENCE: alarmcentrale die bereikbaar is onder het telefoonnummer 112\n",
      "\n",
      "--- Sample 3 ---\n",
      "SOURCE: biljet van 200 euro\n",
      "PREDICTION: <extra_id_0>'. '200 eurobiljet' '200 eurobiljet' '200 eurobiljet' '200 eurobiljet' '200 eurobiljet'  <extra_id_1>.  <extra_id_2>. <extra_id_3>. <extra_id_4>. <extra_id_5>. <extra_id_6>.'  <extra_id_7>. <extra_id_8>. <extra_id_9>.'  <extra_id_10>. <extra_id_11>. <extra_id_12>'. <extra_id_13>. <extra_id_14>. <extra_id_15>. <extra_id_16>. <extra_id_17>.' '200 eurobiljet'  <extra_id_18>. <extra_id_19>. <extra_id_20>. <extra_id_21>.'  <extra_id_22>. <extra_id_23>'.  <extra_id_24>. <extra_id_25>.' '200 eurobiljet' '200 eurobiljet' '200 eurobiljet'  <extra_id_26>. <extra_id_27>.'  <extra_id_28>.'  <extra_id_29>. <extra_id_30>. <extra_id_31>.'  <extra_id_32>. <extra_id_33>. <extra_id_34>.'  <extra_id_35>. <extra_id_36>.'  <extra_id_37>. <extra_id_38>.'  <extra_id_39>. <extra_id_40>. <extra_id_41>.' '200 eurobiljet' ' <extra_id_42>.'  <extra_id_43>.' ''\n",
      "REFERENCE: bankbiljet dat de waarde van 200 euro vertegenwoordigt\n",
      "\n",
      "--- Sample 4 ---\n",
      "SOURCE: monitor met 3D-beeld\n",
      "PREDICTION: <extra_id_0>'. 'monitor met 3D-beeld'. 'monitor met 3D-beeld'. 'monitor met 3D-beeld'.  <extra_id_1>.  <extra_id_2>. <extra_id_3>. <extra_id_4>. <extra_id_5>. <extra_id_6>. <extra_id_7>. <extra_id_8>. <extra_id_9>. <extra_id_10>. <extra_id_11>. <extra_id_12>'. 'monitor met 3D-beeld'.  <extra_id_13>. <extra_id_14>. <extra_id_15>. <extra_id_16>. <extra_id_17>.' 'monitor met 3D-beeld'.  <extra_id_18>. <extra_id_19>. <extra_id_20>. <extra_id_21>'.  <extra_id_22>. <extra_id_23>'.  <extra_id_24>. <extra_id_25>. <extra_id_26>'. 'monitor met 3D-beeld'.  <extra_id_27>. <extra_id_28>.'  <extra_id_29>. <extra_id_30>.'  <extra_id_31>. <extra_id_32>.'  <extra_id_33>. <extra_id_34>.'  <extra_id_35>.'  <extra_id_36>.' ' <extra_id_37>.'  <extra_id_38>. <extra_id_39>.'  <extra_id_40>.'  <extra_id_41>.' ' <extra_id_42>.'  <extra_id_43>.' ' <extra_id_44>.'  <extra_id_45>. <extra_id_46>.' ' <extra_id_47>.' ' <extra_id_48>. <extra_id_49>. <extra_id_51>. <extra_id_52>.' ' <extra_id_53>.' ' <extra_id_54>.'  <extra_id_55>. <extra_id_56>.\n",
      "REFERENCE: monitor die driedimensionaal beeld weergeeft\n",
      "\n",
      "--- Sample 5 ---\n",
      "SOURCE: autootje met bromfietsstatus\n",
      "PREDICTION: <extra_id_0>'. '45 kilometerwagen' '45 kilometerwagen' '45 kilometerwagen' '45 kilometerwagen'  <extra_id_1>'.  <extra_id_2>'.  <extra_id_3>' <extra_id_4>'.  <extra_id_5>' <extra_id_6>' <extra_id_7>'.  <extra_id_8>' <extra_id_9>' <extra_id_10>' <extra_id_11>' <extra_id_12>'.  <extra_id_13>' <extra_id_14>'.  <extra_id_15>' <extra_id_16>'. '45 kilometerwagen'  <extra_id_17>'.\n",
      "REFERENCE: motorvoertuig met de officiële status van een bromfiets en het voorkomen van een kleine auto, dat drie of vier wielen heeft en maximaal 45 km per uur rijdt\n",
      "\n",
      "--- Sample 6 ---\n",
      "SOURCE: Nederlandse politieke partij\n",
      "PREDICTION: <extra_id_0>'. Breid het woord '50PLUS' uit tot een volledige definitie. Breid het woord '50PLUS' uit tot een volledige definitie: ' <extra_id_1>'.  <extra_id_2>. <extra_id_3>. <extra_id_4>. <extra_id_5>. <extra_id_6>. <extra_id_7>. <extra_id_8>. <extra_id_9>. <extra_id_10>. <extra_id_11>. <extra_id_12>. <extra_id_13>. <extra_id_14>. <extra_id_15>. <extra_id_16>. <extra_id_17>'. '50PLUS'. ' <extra_id_18>'. ' <extra_id_19>'. ' <extra_id_20>'. ' <extra_id_21>'. ' <extra_id_22>'. ' <extra_id_23>'. ' <extra_id_24>'. ' <extra_id_25>'. ' <extra_id_26>'. ' <extra_id_27>'. ' <extra_id_28>'. ' <extra_id_29>'. ' <extra_id_30>'. <extra_id_31>. <extra_id_32>. <extra_id_33>. <extra_id_34>.' ' <extra_id_35>'. <extra_id_36>. ' <extra_id_37>'. ' <extra_id_38>'. ' <extra_id_39>'. ' <extra_id_40>'. ' <extra_id_41>'. ' <extra_id_42>' <extra_id_43>'. ' <extra_id_44>' <extra_id_45>' <extra_id_46>' <extra_id_47>.  <extra_id_48>' <extra_id_49>. <extra_id_51>. ' <extra_id_52>'.  <extra_id_53>.  <extra_id_54>.  <extra_id_55>. <extra_id_56>.\n",
      "REFERENCE: niet-ideologische Nederlandse politieke partij die zich richt op ouderen, in 2009 opgericht om 50-plussers meer te vertegenwoordigen in de Nederlandse politiek\n",
      "\n",
      "--- Sample 7 ---\n",
      "SOURCE: zoals (van) met een eigennaam met een soortnaam\n",
      "PREDICTION: <extra_id_0>'. 'zoals (van) met een eigennaam met een soortnaam'. 'zoals (van)'. 'zoals (van)'. 'zoals (van)'. ' <extra_id_1>'. ' <extra_id_2>'. ' <extra_id_3>'. ' <extra_id_4>'. ' <extra_id_5>'. ' <extra_id_6>'. ' <extra_id_7>'. ' <extra_id_8>'. ' <extra_id_9>'. ' <extra_id_10>'. ' <extra_id_11>'. ' <extra_id_12>'. ' <extra_id_13>'. ' <extra_id_14>'. ' <extra_id_15>'. ' <extra_id_16>'. ' <extra_id_17>'. ' <extra_id_18>'. ' <extra_id_19>'. ' <extra_id_43>'. ' <extra_id_44>'. ' <extra_id_45>'. ' <extra_id_46>'. ' <extra_id_47>'. ' <extra_id_47>' <extra_id_51>' <extra_id_52>. <extra_id_53>. 'zoals'. 'zoals'. 'zoals'. 'zoals'. ' <extra_id_54>.' ' <extra_id_43>'\n",
      "REFERENCE: zoals (van) met een eigennaam, hetzij van een persoon, hetzij van een zaak, die als een lichtend of afschrikkend voorbeeld, als het prototype van iets wordt gezien met een soortnaam\n",
      "\n",
      "--- Sample 8 ---\n",
      "SOURCE: in uit het Franse ontleende verbindingen\n",
      "PREDICTION: <extra_id_0>'. 'in uit het Franse ontleende verbindingen'. 'in uit het Franse ontleende verbindingen'. 'in'. ' <extra_id_1>'. 'in'. 'in'. ' <extra_id_2>'. ' <extra_id_3>'. <extra_id_4>. <extra_id_5>. <extra_id_6>. <extra_id_7>. <extra_id_8>. <extra_id_9>. <extra_id_10>. <extra_id_11>. <extra_id_12>. <extra_id_13>. <extra_id_14>. <extra_id_15>. <extra_id_16>'. 'in'. ' <extra_id_17>'. ' <extra_id_18>'. <extra_id_19>. <extra_id_20>. <extra_id_21>'. 'in'. ' <extra_id_22>'. ' <extra_id_23>'. ' <extra_id_24>'. ' <extra_id_25>'.  <extra_id_26>.' ' <extra_id_27>'. ' <extra_id_28>'. ' <extra_id_29>' <extra_id_30>' <extra_id_31>' <extra_id_32>' <extra_id_33>' <extra_id_34>' <extra_id_35>' <extra_id_36>' <extra_id_37>' <extra_id_38>' <extra_id_39>' <extra_id_40>' <extra_id_41>'. ' <extra_id_42>' ' <extra_id_43>' ' <extra_id_44>' ' <extra_id_45>' <extra_id_46>' ' <extra_id_47>' ' <extra_id_48>' <extra_id_49> ' <extra_id_50>' <extra_id_51>' ' <extra_id_52>'  <extra_id_53> ' <extra_id_54>'  <extra_id_55>' <extra_id_56>'\n",
      "REFERENCE: in enige uit het Franse ontleende vaste verbindingen\n",
      "\n",
      "--- Sample 9 ---\n",
      "SOURCE: met een eigennaam\n",
      "PREDICTION: <extra_id_0>'. 'met een eigennaam' 'met een eigennaam' 'met een eigennaam' 'met een eigennaam' 'met een eigennaam' 'met een eigennaam' 'met een eigennaam' 'met een eigennaam'  <extra_id_6>  <extra_id_7>' <extra_id_8>  <extra_id_9>  <extra_id_10>  <extra_id_11>' <extra_id_12>  <extra_id_13>  <extra_id_14> la' <extra_id_15>  <extra_id_16>  <extra_id_17>  <extra_id_18>. <extra_id_19>.  <extra_id_20>.  <extra_id_21>  <extra_id_22>' <extra_id_23> 'met een eigennaam' ' <extra_id_24>' ' <extra_id_25>' 'met een eigennaam' 'met een eigennaam' ' <extra_id_26>' ' <extra_id_31>' ' <extra_id_32>'  <extra_id_33>  <extra_id_34>' ' <extra_id_35>'  <extra_id_36>' '' <extra_id_37>' ' <extra_id_38>' ' <extra_id_39>' ' <extra_id_40>'  <extra_id_41> ' <extra_id_42>' '' <extra_id_43>' ' <extra_id_44>'\n",
      "REFERENCE: met een eigennaam, hetzij van een persoon, hetzij van een zaak, die als een lichtend of afschrikkend voorbeeld, als het prototype van iets wordt gezien\n",
      "\n",
      "--- Sample 10 ---\n",
      "SOURCE: met z'n drieën\n",
      "PREDICTION: <extra_id_0> drieën' 'drieën' 'drieën' 'drieën' 'drieën' 'drieën' 'drieën'  <extra_id_1> drieën' 'drieën' 'drieën' 'drieën'  <extra_id_8> drieën'  <extra_id_9>' <extra_id_10>' <extra_id_11>' <extra_id_12>' <extra_id_13>' <extra_id_14>' <extra_id_15>' <extra_id_16>' <extra_id_17>' <extra_id_18>' <extra_id_19>' <extra_id_20>' <extra_id_21>' 'drieën' 'drieën'\n",
      "REFERENCE: (Frans) met z'n drieën\n",
      "\n",
      "--- Sample 11 ---\n",
      "SOURCE: paling aal als voedsel\n",
      "PREDICTION: <extra_id_0>'. 'paling aal als voedsel'. 'aal'. 'aal'. 'aal'. 'aal'.  <extra_id_1>. <extra_id_2>. <extra_id_3>. <extra_id_4>. <extra_id_5>. <extra_id_6>. <extra_id_7>.' 'aal'. 'aal'. 'aal'.  <extra_id_8>. <extra_id_9>. <extra_id_10>.'  <extra_id_11>. <extra_id_12>.' ' <extra_id_13>'.  <extra_id_14>. <extra_id_15>. <extra_id_16>.' ' <extra_id_17>'.  <extra_id_18>. <extra_id_19>'.  <extra_id_20>. <extra_id_21>\n",
      "REFERENCE: lange, slangachtige vis met een slijmerige, gladde huid, kleine borstvinnen, een lange rugvin die de gehele staart omzoomt en tot de aarsvin reikt, kleine kieuwopeningen en een bovenkaak die korter is dan de onderkaak\n",
      "\n",
      "--- Sample 12 ---\n",
      "SOURCE: aalmoezenier\n",
      "PREDICTION: <extra_id_0>' 'aalmoezenier' 'aalmoezenier' 'aalmoezenier' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal'  <extra_id_6>  <extra_id_7>'  <extra_id_8>  <extra_id_6>' 'a <extra_id_7>'  <extra_id_8>  <extra_id_6>' 'a <extra_id_7>' <extra_id_8>'  <extra_id_9>  <extra_id_10>'  <extra_id_11>' <extra_id_12>' <extra_id_13>' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal'' <extra_id_14>\n",
      "REFERENCE: katholieke geestelijke die verbonden is aan een legerafdeling en die belast is met pastoraal werk en begeleiding en ondersteuning van militairen\n",
      "\n",
      "--- Sample 13 ---\n",
      "SOURCE: mestvocht\n",
      "PREDICTION: <extra_id_0>' 'mestvocht' 'mestvocht' 'mestvocht' 'mestvocht' 'mestvocht' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal'  <extra_id_6>  <extra_id_7>'  <extra_id_8>'  <extra_id_6>'  <extra_id_7>' <extra_id_8>'  <extra_id_6>' <extra_id_7>al <extra_id_8>' ' <extra_id_9>'  <extra_id_10>'  <extra_id_11>' <extra_id_12>  <extra_id_13>'  <extra_id_14>'  <extra_id_15>'  <extra_id_16>'  <extra_id_17>'  <extra_id_18>' <extra_id_19>'  <extra_id_43>' 'mestvocht' ' <extra_id_44>' 'mestvocht' 'mestvocht' 'aal' 'aal'\n",
      "REFERENCE: vloeibare mest\n",
      "\n",
      "--- Sample 14 ---\n",
      "SOURCE: aal als voedsel\n",
      "PREDICTION: <extra_id_0>'. 'aal als voedsel' 'aal als voedsel' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal' 'aal'  <extra_id_6>  <extra_id_7>  <extra_id_8>  <extra_id_6>  <extra_id_7>  <extra_id_10>  <extra_id_11>  <extra_id_12>'  <extra_id_13>  <extra_id_14>  <extra_id_10>  <extra_id_26>' 'a <extra_id_54>' ' <extra_id_55>' 'aal' 'aal'\n",
      "REFERENCE: door de mens als voedsel geconsumeerde aal\n",
      "\n",
      "--- Sample 15 ---\n",
      "SOURCE: zonder enige hobbel of rimpel erg glibberig\n",
      "PREDICTION: <extra_id_0>'. 'zonder enige hobbel of rimpel erg glibberig'. 'zonder enige hobbel of rimpel erg glibberig'.  <extra_id_1>.  <extra_id_2>.'  <extra_id_3>. <extra_id_4>.'  <extra_id_5>.'  <extra_id_6>.'  <extra_id_7>.'  <extra_id_8>.'  <extra_id_9>. <extra_id_10>. <extra_id_11>. <extra_id_12>. <extra_id_13>. <extra_id_14>.'  <extra_id_15>. <extra_id_16>. <extra_id_17>. <extra_id_18>'. ' <extra_id_19>.'  <extra_id_20>.' ' <extra_id_21>'.  <extra_id_22>.' ' <extra_id_23>'.  <extra_id_24>.' ' <extra_id_25>'. ' <extra_id_26>'. ' <extra_id_31>'. ' <extra_id_32>'. ' <extra_id_33>'. ' <extra_id_34>'. ' <extra_id_35>'.  <extra_id_36>.' <extra_id_37>.' ' <extra_id_38>' ' <extra_id_39>' ' <extra_id_40>' ' <extra_id_41>' ' <extra_id_42>' ' <extra_id_43>' ' <extra_id_44>' ' <extra_id_45>' ' <extra_id_46>' ' <extra_id_47>' ' <extra_id_48>' ' <extra_id_49>'  <extra_id_51>' ' <extra_id_52>' ' <extra_id_53>' ' <extra_id_54>' ' <extra_id_55>'  <extra_id_56>\n",
      "REFERENCE: kaal met zeer weinig weerstand, waardoor je kunt uitglijden\n",
      "\n",
      "--- Sample 16 ---\n",
      "SOURCE: supersnel\n",
      "PREDICTION: <extra_id_0> 'aalglad' 'aalglad' 'aalglad' 'aalglad'  <extra_id_1>  <extra_id_2>  <extra_id_3>  <extra_id_4> 'aalglad' 'aalglad'  <extra_id_5>  <extra_id_6>  <extra_id_7>  <extra_id_8>  <extra_id_9>  <extra_id_10>  <extra_id_11>  <extra_id_12>  <extra_id_13>  <extra_id_14> 'aalglad'  <extra_id_15>  <extra_id_16>  <extra_id_17> 'aalglad'  <extra_id_18>  <extra_id_19>  <extra_id_20> 'aalglad'  <extra_id_21>  <extra_id_22>  <extra_id_23>'\n",
      "REFERENCE: niet te verslaan\n",
      "\n",
      "--- Sample 17 ---\n",
      "SOURCE: niet te vangen\n",
      "PREDICTION: <extra_id_0> vangen' 'niet te vangen' 'niet te vangen' 'niet te vangen' 'niet te vangen'  <extra_id_1> vangen' ' <extra_id_2>'  <extra_id_3>' <extra_id_4>' <extra_id_5>' <extra_id_6>' <extra_id_7>' <extra_id_8>' <extra_id_9>' <extra_id_10>' <extra_id_11>' <extra_id_12>' <extra_id_13>' <extra_id_14>' <extra_id_15>' <extra_id_16>' 'niet te vangen' 'niet te vangen'  <extra_id_17>' <extra_id_18>' <extra_id_19>' <extra_id_20>' <extra_id_21>' <extra_id_22>' <extra_id_23>' 'niet te vangen' ' <extra_id_24>' ' <extra_id_25>' 'niet te vangen' ' <extra_id_26>' ' <extra_id_31>' ' <extra_id_32>' '''''''' <extra_id_33>' <extra_id_34>' <extra_id_35>' <extra_id_36>' <extra_id_37>'''''' <extra_id_38>' <extra_id_39>''''''' <extra_id_40>' <extra_id_41>' <extra_id_42>'' <extra_id_43>''''''''''''''''\n",
      "REFERENCE: communicatief zeer sterk\n",
      "\n",
      "--- Sample 18 ---\n",
      "SOURCE: (te) gelikt\n",
      "PREDICTION: <extra_id_0> aalglad aalglad aalglad aalglad aalglad aalglad aalglad aalglad' <extra_id_1>...\n",
      "REFERENCE: goed in elkaar zittend\n",
      "\n",
      "--- Sample 19 ---\n",
      "SOURCE: erg glibberig\n",
      "PREDICTION: <extra_id_0>'. 'erg glibberig'. 'erg glibberig'. 'aalglad'.  <extra_id_1>.  <extra_id_2>.'.\n",
      "REFERENCE: met zeer weinig weerstand, waardoor je kunt uitglijden\n",
      "\n",
      "--- Sample 20 ---\n",
      "SOURCE: iemand uit Aalst lid e.d. van een club uit Aalst\n",
      "PREDICTION: <extra_id_0>'. 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar'  <extra_id_1> Aalst Aalstenaar A <extra_id_2> A <extra_id_3> Aalstenaar A <extra_id_4> Aalstenaar A <extra_id_5> A <extra_id_6>  <extra_id_7>  <extra_id_8>  <extra_id_9>  <extra_id_10>  <extra_id_11>  <extra_id_12>  <extra_id_13>  <extra_id_14>  <extra_id_15>  <extra_id_16>  <extra_id_17>  <extra_id_18>  <extra_id_19>  <extra_id_20>  <extra_id_21>  <extra_id_22>  <extra_id_23>  <extra_id_24>  <extra_id_25>  <extra_id_26>  <extra_id_27>  <extra_id_28>  <extra_id_29>  <extra_id_30>  <extra_id_31>  <extra_id_32>  <extra_id_33>  <extra_id_34>  <extra_id_35>  <extra_id_36>  <extra_id_37>  <extra_id_38>  <extra_id_39>  <extra_id_40>  <extra_id_41>  <extra_id_42>  <extra_id_43>  <extra_id_44>  <extra_id_45>  <extra_id_46>  <extra_id_47>  <extra_id_48>  <extra_id_49> ' <extra_id_50> ' ' <extra_id_51> ' ' <extra_id_52> ' ' <extra_id_53> ' <extra_id_54> '  <extra_id_55> ' <extra_id_56> \n",
      "REFERENCE: inwoner van de stad Aalst iemand die lid of aanhanger is van een sportclub, vereniging of andere groep uit Aalst\n",
      "\n",
      "--- Sample 21 ---\n",
      "SOURCE: lid e.d. van een club uit Aalst\n",
      "PREDICTION: <extra_id_0>'. 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar'  <extra_id_6>'  <extra_id_7>  <extra_id_8>  <extra_id_9>  <extra_id_10>  <extra_id_11>  <extra_id_12>  <extra_id_13>  <extra_id_14>  <extra_id_15>  <extra_id_16>  <extra_id_17>  <extra_id_18>  <extra_id_19>  <extra_id_20>  <extra_id_21>  <extra_id_36>  <extra_id_37>  <extra_id_38> 'Aalstenaar' 'Aalstenaar' 'Aalstenaar' 'Aalstenaar'  <extra_id_39>  <extra_id_40>  <extra_id_41>  <extra_id_42>  <extra_id_43>  <extra_id_44>  <extra_id_45>  <extra_id_46>  <extra_id_47>  <extra_id_48>  <extra_id_49> 'lid' 'lid'\n",
      "REFERENCE: iemand die lid of aanhanger is van een sportclub, vereniging of andere groep uit Aalst\n",
      "\n",
      "--- Sample 22 ---\n",
      "SOURCE: zanger van religieuze liederen\n",
      "PREDICTION: <extra_id_0>'. 'aanbiddingszanger'. 'aanbiddingszanger'. 'aanbiddingszanger'. ' <extra_id_1>'. ' <extra_id_2>'. ' <extra_id_3>'. ' <extra_id_4>'. ' <extra_id_5>'. ' <extra_id_6>'.  <extra_id_7>.  <extra_id_8>.'  <extra_id_9>. <extra_id_10>.'  <extra_id_11>. <extra_id_12>. <extra_id_13>. <extra_id_14>. <extra_id_15>. <extra_id_16>. <extra_id_17>.' ' <extra_id_18>'. <extra_id_19>. <extra_id_20>. <extra_id_21>.' ' <extra_id_22>'. ' <extra_id_23>'. ' <extra_id_24>'. ' <extra_id_25>'. ' <extra_id_26>'. ' <extra_id_27>'. ' <extra_id_28>'. ' <extra_id_29>'. ' <extra_id_30>'. ' <extra_id_31>' ' <extra_id_32>' ' <extra_id_33>' <extra_id_34>' ' <extra_id_35>' <extra_id_36>' ' <extra_id_37>' ' <extra_id_38>' ' <extra_id_39>' ' <extra_id_40>' <extra_id_41> ' <extra_id_42>\n",
      "REFERENCE: zanger die religieuze, meestal christelijke, liederen zingt\n",
      "\n",
      "--- Sample 23 ---\n",
      "SOURCE: premie voor een functionaris die aanblijft\n",
      "PREDICTION: <extra_id_0>'. 'aanblijfpremie' 'aanblijfpremie' 'aanblijfpremie' 'aanblijfpremie' 'aanblijfpremie' 'aanblijfpremie' 'aanblijfpremie' 'aanblijfpremie'  <extra_id_6>  <extra_id_7>  <extra_id_8>'  <extra_id_6>  <extra_id_7>  <extra_id_8>  <extra_id_9>  <extra_id_10>  <extra_id_11>  <extra_id_12>  <extra_id_13>  <extra_id_14>  <extra_id_15>  <extra_id_16>  <extra_id_17>  <extra_id_18>  <extra_id_19>  <extra_id_20>  <extra_id_21> 'aanblijfpremie' 'aanblijfpremie' ' <extra_id_36>' 'aanblijfpremie' 'aanblijfpremie' 'aanblijfpremie'  <extra_id_25>  <extra_id_26>' 'aanblijfpremie' ''\n",
      "REFERENCE: premie die uitgeloofd wordt als iemand aanblijft in een bepaalde functie, meestal een belangrijke functie\n",
      "\n",
      "--- Sample 24 ---\n",
      "SOURCE: mate waarin iemand geconcentreerd blijft\n",
      "PREDICTION: <extra_id_0> 'aandachtsboog' 'aandachtsboog' 'aandachtsboog' 'aandachtsboog' 'aandachtsboog' 'aandachtsboog' 'aandachtsboog' 'aandachtsboog' ' <extra_id_1>' ' <extra_id_2>' ' <extra_id_3>' ' <extra_id_4>' ' <extra_id_5>' ' <extra_id_6>' ' <extra_id_7>' ' <extra_id_8>' ' <extra_id_9>' ' <extra_id_10>' ' <extra_id_11>' ' <extra_id_12>' ' <extra_id_13>' ' <extra_id_14>' ' <extra_id_15>' ' <extra_id_16>' ' <extra_id_17>' ' <extra_id_18>' ' <extra_id_19>' ' <extra_id_20>' ' <extra_id_21>' ' <extra_id_22>' ' <extra_id_23>  <extra_id_31>' ' <extra_id_32>' ' <extra_id_33>' ' <extra_id_34>' ' <extra_id_35>' ' <extra_id_36>' ' <extra_id_37>' ' <extra_id_38>' ' <extra_id_39>' ' <extra_id_40>' <extra_id_41>' ' <extra_id_42>' <extra_id_43>' <extra_id_43>' <extra_id_43>' <extra_id_43>' ' <extra_id_44>' <extra_id_45>' <extra_id_49>'\n",
      "REFERENCE: mate waarin of tijd gedurende welke iemand geconcentreerd blijft of zijn aandacht ergens bij kan houden, bijvoorbeeld om te lezen, luisteren, studeren, etc.\n",
      "\n",
      "--- Sample 25 ---\n",
      "SOURCE: economie gericht op aandacht\n",
      "PREDICTION: <extra_id_0>economie'. 'economie gericht op aandacht' 'economie gericht op aandacht' 'economie gericht op aandacht' 'economie gericht op aandacht' 'economie gericht op aandacht' ' <extra_id_1>' 'economie gericht op aandacht' 'economie gericht op aandacht' 'economie gericht op aandacht'  <extra_id_8>  <extra_id_6>  <extra_id_7>  <extra_id_8>  <extra_id_9>  <extra_id_10>  <extra_id_11>  <extra_id_12>  <extra_id_13>  <extra_id_14>  <extra_id_15>  <extra_id_16>  <extra_id_17>  <extra_id_18>  <extra_id_19>  <extra_id_20>  <extra_id_21>  <extra_id_36>  <extra_id_37> 'economie gericht op aandacht' 'economie gericht op aandacht' 'economie gericht op aandacht'  <extra_id_6>  <extra_id_26>'  <extra_id_38>  <extra_id_39>  <extra_id_40>  <extra_id_41>  <extra_id_42>'  <extra_id_43>  <extra_id_44>  <extra_id_45>  <extra_id_46>  <extra_id_47>  <extra_id_47>  <extra_id_47>  <extra_id_47> 'economie gericht op aandacht'\n",
      "REFERENCE: economie of samenleving waarin het trekken en behouden van aandacht van consumenten of mensen in het algemeen centraal staat\n",
      "\n",
      "--- Sample 26 ---\n",
      "SOURCE: mate waarin iemand geconcentreerd blijft\n",
      "PREDICTION: <extra_id_0>'. 'aandachtsspanne' 'aandachtsspanne' 'aandachtsspanne' 'aandachtsspanne' 'aandachtsspanne' 'aandachtsspanne' 'aandachtsspanne' 'aandachtsspanne' ' <extra_id_1>' ' <extra_id_2>' ' <extra_id_3>' ' <extra_id_4>' ' <extra_id_5>' ' <extra_id_6>' ' <extra_id_7>' ' <extra_id_8>' ' <extra_id_9>' ' <extra_id_10>' ' <extra_id_11>' ' <extra_id_12>' ' <extra_id_13>' ' <extra_id_14>' ' <extra_id_15>' ' <extra_id_16>' ' <extra_id_17>' ' <extra_id_18>' ' <extra_id_19>' ' <extra_id_20>' ' <extra_id_21>' ' <extra_id_22>' ' <extra_id_23>' ' <extra_id_24>' ' <extra_id_25>' ' <extra_id_26>' ' <extra_id_27>' ' <extra_id_42>' ' <extra_id_43>' ' <extra_id_44>' ' <extra_id_45>'  <extra_id_31>' ' <extra_id_31>' ' <extra_id_31>' ' <extra_id_31>' ' <extra_id_31>' '\n",
      "REFERENCE: mate waarin of tijd gedurende welke iemand geconcentreerd blijft of zijn aandacht ergens bij kan houden, bijvoorbeeld om te lezen, luisteren, studeren, etc.\n",
      "\n",
      "--- Sample 27 ---\n",
      "SOURCE: waarde van een onderneming\n",
      "PREDICTION: <extra_id_0>'. 'aandeelhouderswaarde' 'aandeelhouderswaarde' 'aandeelhouderswaarde' 'onderneming' 'onderneming' 'onderneming' 'onderneming' 'onderneming'  <extra_id_1> 'onderneming' 'onderneming' 'onderneming' 'onderneming' 'onderneming'  <extra_id_6>' <extra_id_7>' <extra_id_8>' '\n",
      "REFERENCE: waarde die een onderneming of een bedrijf vertegenwoordigt in de ogen van mogelijke investeerders en die vooral wordt beïnvloed door de omvang en de winstgevendheid van die onderneming of dat bedrijf\n",
      "\n",
      "--- Sample 28 ---\n",
      "SOURCE: beleggingsproduct gekocht met geleend geld\n",
      "PREDICTION: <extra_id_0> aandelenleaseproduct aandelenleaseproduct aandelenleaseproduct aandelenleaseproduct aandelenleaseproduct aandelenleaseproduct  <extra_id_1> aandelenleaseproduct  <extra_id_2>  <extra_id_3>  <extra_id_4> aandelenleaseproduct  <extra_id_5>  <extra_id_6>  <extra_id_7> aandelenleaseproduct  <extra_id_8>  <extra_id_9>  <extra_id_10>  <extra_id_11>  <extra_id_12>  <extra_id_13>  <extra_id_14>  <extra_id_15>  <extra_id_16> aandelenleaseproduct  <extra_id_17> aandelenleaseproduct  <extra_id_18>  <extra_id_19>  <extra_id_20>  <extra_id_21>  <extra_id_22> aandelenleaseproduct  <extra_id_23> aandelenleaseproduct  <extra_id_24>  <extra_id_25>  <extra_id_26> aandelenleaseproduct  <extra_id_27>  <extra_id_28>  <extra_id_29>  <extra_id_30> aandelenleaseproduct  <extra_id_31>  <extra_id_32>  <extra_id_33> aandelenleaseproduct  <extra_id_34>  <extra_id_35>  <extra_id_36> aandelenleaseproduct  <extra_id_37> aandelenleaseproduct' ' <extra_id_38>' ' <extra_id_39>' ' <extra_id_40>' ' <extra_id_41>' ' <extra_id_42>' ' <extra_id_43>' ' <extra_id_44>' ' <extra_id_45>' ' <extra_id_46>' ' <extra_id_47>' '\n",
      "REFERENCE: financieel product waarbij er belegd wordt met aandelen die gekocht zijn met geleend geld\n",
      "\n",
      "--- Sample 29 ---\n",
      "SOURCE: datum waarop iemand of iets aankomt\n",
      "PREDICTION: <extra_id_0>'. 'datum waarop iemand of iets aankomt' 'datum waarop iets aankomt' 'aankomstdatum' 'datum' 'datum' 'datum' 'datum' 'datum' 'datum' 'datum' 'datum' 'datum' 'datum' 'datum' 'datum' 'datum' 'datum'  <extra_id_6> ' <extra_id_7>  <extra_id_8>  <extra_id_6>  <extra_id_7>  <extra_id_8>  <extra_id_10>  <extra_id_11>  <extra_id_12>  <extra_id_13>  <extra_id_14>  <extra_id_15>  <extra_id_16>  <extra_id_17>  <extra_id_36> 'datum' 'datum' 'datum' ' <extra_id_37>  <extra_id_38>  <extra_id_39>  <extra_id_40>  <extra_id_41> 'datum' 'datum' ' <extra_id_42>  <extra_id_43> 'datum' 'datum' ' <extra_id_42>' ' <extra_id_43>' ' <extra_id_42>' ' <extra_id_43>' ' <extra_id_42>' ' <extra_id_43>' '\n",
      "REFERENCE: datum waarop iemand (of een groep van personen) of iets geacht wordt of werd aan te komen\n",
      "\n",
      "--- Sample 30 ---\n",
      "SOURCE: fraude bij aankopen\n",
      "PREDICTION: <extra_id_0>'. 'aankoopfraude' 'aankoopfraude' 'aankoopfraude' 'aankoopfraude' ' <extra_id_1>'. <extra_id_2>. <extra_id_3>. <extra_id_4>. <extra_id_5>. <extra_id_6>. <extra_id_7>. <extra_id_8>. <extra_id_9>. <extra_id_10>. <extra_id_11>. <extra_id_12>'. 'aankoopfraude' 'aankoopfraude' ' <extra_id_13>.'  <extra_id_14>. <extra_id_15>. <extra_id_16>.' ' <extra_id_17>'.  <extra_id_18>. <extra_id_19>'.  <extra_id_20>. <extra_id_21>'.  <extra_id_22>. <extra_id_23>'. ' <extra_id_24>'. ' <extra_id_25>'. ' <extra_id_26>'. ' <extra_id_27>'. ' <extra_id_28>'. ' <extra_id_29>'. ' <extra_id_30>'.  <extra_id_31>'. <extra_id_32>'.  <extra_id_33>. <extra_id_34>'. ' <extra_id_35>'. <extra_id_36>. <extra_id_37>'. ' <extra_id_38>'.  <extra_id_39>'. ' <extra_id_40>'.  <extra_id_41>'. ' <extra_id_42>' <extra_id_43>'. ' <extra_id_44>' ' <extra_id_45>' <extra_id_46>' <extra_id_47>' <extra_id_48>' <extra_id_49>' <extra_id_51>' ' <extra_id_52>' ' <extra_id_53>' ' <extra_id_54>' ' <extra_id_55>' <extra_id_56>'\n",
      "REFERENCE: vorm van fraude die gepleegd wordt bij de aankoop van goederen of diensten, waarbij deze meestal niet geleverd worden, terwijl de klant er wel voor betaald heeft\n",
      "\n",
      "--- Sample 31 ---\n",
      "SOURCE: brief met een aanmaning\n",
      "PREDICTION: <extra_id_0>'. 'brief met een aanmaning' 'brief met een aanmaning' 'brief met een aanmaning' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief' 'brief'  <extra_id_8> ' <extra_id_10>  <extra_id_11> ' <extra_id_51>' ' <extra_id_33>'\n",
      "REFERENCE: brief waarin de ontvanger aangemaand wordt iets te doen, bijvoorbeeld een openstaande rekening te betalen, een besteld product te leveren, een belofte na te komen e.d.\n",
      "\n",
      "--- Sample 32 ---\n",
      "SOURCE: moeilijkheden bij het zich aanpassen\n",
      "PREDICTION: <extra_id_0>'. 'moeilijkheden bij het zich aanpassen' 'moeilijkheden bij het zich aanpassen' 'moeilijkheden bij het zich aanpassen' 'moeilijkheden bij het zich aanpassen' ' <extra_id_1>' 'moeilijkheden bij het zich aanpassen' 'moeilijkheden bij het zich aanpassen'  <extra_id_8>  <extra_id_9>  <extra_id_10>  <extra_id_11>  <extra_id_12>  <extra_id_13>  <extra_id_14>  <extra_id_15>  <extra_id_16>  <extra_id_17>  <extra_id_18>  <extra_id_19>  <extra_id_20>  <extra_id_21>  <extra_id_22>  <extra_id_23>  <extra_id_24>  <extra_id_25>  <extra_id_26>' 'moeilijkheden bij het zich aanpassen' ' <extra_id_31>' ' <extra_id_32>'  <extra_id_33>  <extra_id_34> ' <extra_id_35>  <extra_id_36>' ' <extra_id_37>' ' <extra_id_38>' ' <extra_id_39>' ' <extra_id_40>' ' <extra_id_41>' ' <extra_id_42>' ' <extra_id_43>' ' ' <extra_id_44>' ' <extra_id_45>'  <extra_id_46>' <extra_id_47>' <extra_id_49>' <extra_id_51>' ' <extra_id_52>' 'moeilijkheden bij het zich aanpassen' '  <extra_id_53> '  <extra_id_56> \n",
      "REFERENCE: moeilijkheden die zich voordoen als iemand zich probeert aan te passen aan veranderde omstandigheden\n",
      "\n",
      "--- End of First Batch ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39mMAX_SOURCE_LENGTH)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_TARGET_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Decode predictions based on model type\u001b[39;00m\n\u001b[1;32m    116\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2646\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2647\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2648\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2649\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2650\u001b[0m     )\n\u001b[1;32m   2651\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2652\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2657\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2662\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   2663\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2665\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:4088\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   4085\u001b[0m beam_indices \u001b[38;5;241m=\u001b[39m running_beam_indices\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m   4087\u001b[0m \u001b[38;5;66;03m# 4. run the generation loop\u001b[39;00m\n\u001b[0;32m-> 4088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;66;03m# a. Forward current tokens, obtain the logits\u001b[39;00m\n\u001b[1;32m   4090\u001b[0m     flat_running_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[1;32m   4091\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(flat_running_sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2778\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   2775\u001b[0m         result\u001b[38;5;241m.\u001b[39mpast_key_values \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mpast_key_values\u001b[38;5;241m.\u001b[39mto_legacy_cache()\n\u001b[1;32m   2776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 2778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_has_unfinished_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, this_peer_finished: \u001b[38;5;28mbool\u001b[39m, synced_gpus: \u001b[38;5;28mbool\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   2779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;124;03m    Returns whether there are still unfinished sequences in the device. The existence of unfinished sequences is\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m \u001b[38;5;124;03m    fed through `this_peer_finished`. ZeRO stage 3-friendly.\u001b[39;00m\n\u001b[1;32m   2782\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m synced_gpus:\n\u001b[1;32m   2784\u001b[0m         \u001b[38;5;66;03m# Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\u001b[39;00m\n\u001b[1;32m   2785\u001b[0m         \u001b[38;5;66;03m# The following logic allows an early break if all peers finished generating their sequence\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Main Evaluation Loop ---\n",
    "for model_id in MODEL_IDS:\n",
    "    print(f\"\\n{'='*20}\\nEVALUATING MODEL: {model_id}\\n{'='*20}\")\n",
    "\n",
    "    # --- 1. Set Model-Specific Configuration ---\n",
    "    if model_id in [\"bramvanroy/geitje-7b-ultra\", \"CohereLabs/aya-expanse-8b\"]:\n",
    "        MAX_SOURCE_LENGTH = 512\n",
    "        MAX_TARGET_LENGTH = 512\n",
    "        BATCH_SIZE = 32\n",
    "    elif model_id == \"CohereLabs/aya-101\":\n",
    "        MAX_SOURCE_LENGTH = 384\n",
    "        MAX_TARGET_LENGTH = 384\n",
    "        BATCH_SIZE = 16\n",
    "    elif model_id == \"google/mt5-xl\":\n",
    "        MAX_SOURCE_LENGTH = 512\n",
    "        MAX_TARGET_LENGTH = 512\n",
    "        BATCH_SIZE = 32\n",
    "\n",
    "    # --- 2. Load Model and Tokenizer ---\n",
    "    print(f\"Loading {model_id}...\")\n",
    "    \n",
    "    # Create the correct bnb_config based on the model\n",
    "    if model_id == \"CohereLabs/aya-101\":\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True, # Specific to this model\n",
    "        )\n",
    "    else:\n",
    "        # Default config for other models\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "\n",
    "    # Load the model based on its type\n",
    "    if model_id in [\"google/mt5-xl\", \"CohereLabs/aya-101\"]:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    elif model_id == \"CohereLabs/aya-expanse-8b\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_id,\n",
    "          quantization_config=bnb_config,\n",
    "          device_map=\"auto\",\n",
    "          attn_implementation=\"sdpa\",\n",
    "          trust_remote_code=True,\n",
    "      )\n",
    "    elif model_id == \"bramvanroy/geitje-7b-ultra\": \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "\n",
    "    # --- 3. Run Evaluation for each Shot Type ---\n",
    "    for shot_type in [\"zero_shot\"]:\n",
    "        if shot_type != \"zero_shot\" and not few_shot_dict:\n",
    "            print(f\"Skipping {shot_type} because few-shot examples were not found.\")\n",
    "            continue\n",
    "\n",
    "        # --- 3a. Generation Loop ---\n",
    "        results = []\n",
    "        print(f\"\\nGenerating predictions for {len(dataset)} samples using {model_id} ({shot_type})...\")\n",
    "        \n",
    "        for i in tqdm(range(0, len(dataset), BATCH_SIZE)):\n",
    "            batch = dataset[i:i+BATCH_SIZE]\n",
    "            prompts = [\n",
    "                create_prompt(model_id, tokenizer, shot_type, lemma, gloss, one_shot_example, few_shot_dict)\n",
    "                for lemma, gloss in zip(batch['Lemma'], batch['DefinitionShort'])\n",
    "            ]\n",
    "            \n",
    "            # Define sources for COMET metric based on the model\n",
    "            sources = []\n",
    "            if model_id in [\"CohereLabs/aya-101\", \"google/mt5-xl\"]:\n",
    "                sources = prompts\n",
    "            elif model_id == \"CohereLabs/aya-expanse-8b\":\n",
    "                sources = [\n",
    "                    f\"Breid de volgende korte definitie voor het woord '{lemma}' uit tot een volledige definitie: '{short_def}'\"\n",
    "                    for lemma, short_def in zip(batch['Lemma'], batch['DefinitionShort'])\n",
    "                ]\n",
    "            elif model_id == \"bramvanroy/geitje-7b-ultra\":\n",
    "                #sources = [p.split(\"<|user|>\")[1].split(\"<|assistant|>\")[0].strip() for p in prompts]\n",
    "                sources = [\n",
    "                    f\"Breid de volgende korte definitie voor het woord '{lemma}' uit tot een volledige definitie: '{short_def}'\"\n",
    "                    for lemma, short_def in zip(batch['Lemma'], batch['DefinitionShort'])\n",
    "                ]\n",
    "\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SOURCE_LENGTH).to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=MAX_TARGET_LENGTH,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            # Decode predictions based on model type\n",
    "            predictions = []\n",
    "            if model_id in [\"CohereLabs/aya-101\", \"google/mt5-xl\"]:\n",
    "                # For Seq2Seq models (like google/mt5-xl), the output is the prediction.\n",
    "                predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            elif model_id == \"bramvanroy/geitje-7b-ultra\":\n",
    "                # For Geitje, we decode the full output and split by the instruction token.\n",
    "                #decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "                #predictions = [decoded.split(\"<|assistant|>\")[1].strip() for decoded in decoded_outputs]\n",
    "                # Ask generate() to give us the full sequence tensor back\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=MAX_TARGET_LENGTH,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        return_dict_in_generate=True,   # NEW\n",
    "                        output_scores=False             # keeps the object small\n",
    "                    )\n",
    "\n",
    "                # Everything up to len(prompt) tokens is the prompt itself,\n",
    "                # so slice it away before decoding.\n",
    "                prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "                gen_only = outputs.sequences[:, prompt_len:]\n",
    "\n",
    "                decoded_outputs = tokenizer.batch_decode(gen_only,\n",
    "                                                         skip_special_tokens=True)\n",
    "                predictions = [txt.strip() for txt in decoded_outputs]\n",
    "\n",
    "            elif model_id == \"CohereLabs/aya-expanse-8b\":\n",
    "                decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                for decoded_output in decoded_outputs:\n",
    "                    try:\n",
    "                        # Split the output at the chatbot token to isolate the generated text\n",
    "                        prediction_part = decoded_output.split(\"<|CHATBOT_TOKEN|>\")[1]\n",
    "                        # Clean up by removing the end-of-turn token\n",
    "                        prediction = prediction_part.replace(\"<|END_OF_TURN_TOKEN|>\", \"\").strip()\n",
    "                        predictions.append(prediction)\n",
    "                    except IndexError:\n",
    "                        # Handle cases where the model fails to generate the expected token\n",
    "                        predictions.append(\"\")\n",
    "\n",
    "            # *** PRINT FIRST BATCH RESULTS ***\n",
    "            if i == 0:\n",
    "                print(\"\\n--- First Batch Generation Results ---\")\n",
    "                for k in range(len(predictions)):\n",
    "                    print(f\"\\n--- Sample {k+1} ---\")\n",
    "                    print(f\"SOURCE: {batch['DefinitionShort'][k]}\")\n",
    "                    print(f\"PREDICTION: {predictions[k]}\")\n",
    "                    print(f\"REFERENCE: {batch['DefinitionFull'][k]}\")\n",
    "                print(\"\\n--- End of First Batch ---\")\n",
    "\n",
    "            for j in range(len(predictions)):\n",
    "                results.append({\n",
    "                    \"source\": sources[j],\n",
    "                    \"prediction\": predictions[j],\n",
    "                    \"reference\": batch['DefinitionFull'][j]\n",
    "                })\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(\"Generation complete.\")\n",
    "        \n",
    "\n",
    "        # --- 3b. Metric Calculation and Saving ---\n",
    "        print(f\"Calculating and saving metrics for {shot_type}...\")\n",
    "        # ==============================================================================\n",
    "        # 4. CALCULATE AND DISPLAY METRICS\n",
    "        # ==============================================================================\n",
    "\n",
    "        model_name_map = {\n",
    "            \"CohereLabs/aya-101\": \"aya-101\",\n",
    "            \"google/mt5-xl\": \"mt5-xl\",\n",
    "            \"bramvanroy/geitje-7b-ultra\": \"Geitje\",\n",
    "            \"CohereLabs/aya-expanse-8b\": \"aya-23\"\n",
    "        }\n",
    "        short_model_name = model_name_map.get(model_id, \"unknown_model\")\n",
    "\n",
    "        # Extract lists of predictions, references, and sources from the DataFrame\n",
    "        predictions = results_df[\"prediction\"].tolist()\n",
    "        references = results_df[\"reference\"].tolist()\n",
    "        sources = results_df[\"source\"].tolist()\n",
    "\n",
    "        final_results_df = dataset.to_pandas().iloc[:len(results_df)]\n",
    "\n",
    "        # Add the model's predictions from the generation step\n",
    "        final_results_df['model_prediction'] = results_df['prediction'].values\n",
    "        # --- Save to TSV File ---\n",
    "        tsv_path = f\"./evaluation_results_per_entry_{short_model_name}_{shot_type}.tsv\"\n",
    "        final_results_df.to_csv(tsv_path, sep='\\t', index=False)\n",
    "\n",
    "        # --- ROUGE ---\n",
    "        rouge = evaluate.load('rouge')\n",
    "        rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "        print(\"\\n--- ROUGE Scores ---\")\n",
    "        print(rouge_results)\n",
    "\n",
    "        # --- BLEU ---\n",
    "        bleu = evaluate.load('bleu')\n",
    "        # Note: BLEU expects references to be a list of lists\n",
    "        bleu_results = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "        print(\"\\n--- BLEU Score ---\")\n",
    "        print(bleu_results)\n",
    "\n",
    "        # --- BERTScore ---\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "        bertscore_results = bertscore.compute(predictions=predictions, references=references, lang=\"nl\")\n",
    "\n",
    "        avg_precision = sum(bertscore_results['precision']) / len(bertscore_results['precision'])\n",
    "        avg_recall = sum(bertscore_results['recall']) / len(bertscore_results['recall'])\n",
    "        avg_f1 = sum(bertscore_results['f1']) / len(bertscore_results['f1'])\n",
    "\n",
    "        print(f\"\\n--- BERTScore ---\")\n",
    "        print(f\"{'Average Precision':>20}: {avg_precision:.4f}\")\n",
    "        print(f\"{'Average Recall':>20}: {avg_recall:.4f}\")\n",
    "        print(f\"{'Average F1':>20}: {avg_f1:.4f}\")\n",
    "\n",
    "        # --- COMET ---\n",
    "        print(\"\\n--- COMET Scores ---\")\n",
    "        print(\"Loading COMET models (this may take a while)...\")\n",
    "        comet_22 = evaluate.load('comet', 'Unbabel/wmt22-comet-da')\n",
    "        comet_kiwi = evaluate.load('comet', 'Unbabel/wmt22-cometkiwi-da')\n",
    "        xcomet = evaluate.load('comet', 'Unbabel/XCOMET-XL')\n",
    "\n",
    "        comet_22_results = comet_22.compute(predictions=predictions, references=references, sources=sources)\n",
    "        comet_kiwi_results = comet_kiwi.compute(predictions=predictions, references=references, sources=sources)\n",
    "        xcomet_results = xcomet.compute(predictions=predictions, references=references, sources=sources)\n",
    "\n",
    "        print(f\"\\n--- COMET-22 Score ---\")\n",
    "        print(f\"{'score':>20}: {comet_22_results['mean_score']:.4f}\")\n",
    "        print(f\"\\n--- COMETkiwi Score ---\")\n",
    "        print(f\"{'score':>20}: {comet_kiwi_results['mean_score']:.4f}\")\n",
    "        print(f\"\\n--- XCOMET Score ---\")\n",
    "        print(f\"{'score':>20}: {xcomet_results['mean_score']:.4f}\")\n",
    "\n",
    "\n",
    "        # ==============================================================================\n",
    "        # 5. SAVE RESULTS TO DISK\n",
    "        # ==============================================================================\n",
    "        print(\"\\nSaving results to disk...\")\n",
    "\n",
    "        # --- Save the summary scores to a text file ---\n",
    "        summary_path = f\"./evaluation_summary_{short_model_name}_{shot_type}.txt\"\n",
    "        with open(summary_path, \"w\") as f:\n",
    "            f.write(\"--- ROUGE Scores ---\\n\")\n",
    "            f.write(str(rouge_results) + \"\\n\")\n",
    "            f.write(\"\\n--- BLEU Score ---\\n\")\n",
    "            f.write(str(bleu_results) + \"\\n\")\n",
    "            f.write(\"\\n--- BERTScore ---\\n\")\n",
    "            f.write(f\"{'Average Precision':>20}: {avg_precision:.4f}\\n\")\n",
    "            f.write(f\"{'Average Recall':>20}: {avg_recall:.4f}\\n\")\n",
    "            f.write(f\"{'Average F1':>20}: {avg_f1:.4f}\\n\")\n",
    "            f.write(\"\\n--- COMET-22 Score ---\\n\")\n",
    "            f.write(f\"{'score':>20}: {comet_22_results['mean_score']:.4f}\\n\")\n",
    "            f.write(\"\\n--- COMETkiwi Score ---\\n\")\n",
    "            f.write(f\"{'score':>20}: {comet_kiwi_results['mean_score']:.4f}\\n\")\n",
    "            f.write(\"\\n--- XCOMET Score ---\\n\")\n",
    "            f.write(f\"{'score':>20}: {xcomet_results['mean_score']:.4f}\\n\")\n",
    "\n",
    "        print(f\"Summary of scores saved to: {summary_path}\")\n",
    "\n",
    "        # ==============================================================================\n",
    "        # 6. CREATE AND SAVE DETAILED PER-ENTRY RESULTS TSV\n",
    "        # ==============================================================================\n",
    "        print(\"--- Creating a detailed results file with per-entry scores ---\")\n",
    "\n",
    "        try:\n",
    "            # Convert the original test set to a pandas DataFrame\n",
    "            # We need to slice the dataset to match the entries in results_df\n",
    "            # This is a safe way to handle it, though results_df should cover the whole dataset\n",
    "            final_results_df = dataset.to_pandas().iloc[:len(results_df)]\n",
    "\n",
    "            # Add the model's predictions from the generation step\n",
    "            final_results_df['model_prediction'] = results_df['prediction'].values\n",
    "\n",
    "            # --- Add Per-Entry Metric Scores ---\n",
    "            # Add BERTScore (Precision, Recall, and F1)\n",
    "            final_results_df['bertscore_precision'] = bertscore_results['precision']\n",
    "            final_results_df['bertscore_recall'] = bertscore_results['recall']\n",
    "            final_results_df['bertscore_f1'] = bertscore_results['f1']\n",
    "            \n",
    "            # Add COMET-22, CometKiwi and XCOMET scores\n",
    "            final_results_df['comet22_score'] = comet_22_results['scores']\n",
    "            final_results_df['cometkiwi_score'] = comet_kiwi_results['scores']\n",
    "            final_results_df['xcomet_score'] = xcomet_results['scores']\n",
    "\n",
    "            # --- Save to TSV File ---\n",
    "            tsv_path = f\"./evaluation_results_per_entry_{short_model_name}_{shot_type}.tsv\"\n",
    "            final_results_df.to_csv(tsv_path, sep='\\t', index=False)\n",
    "\n",
    "            print(f\"\\nSuccessfully created and saved the detailed results file.\")\n",
    "            print(f\"File saved to: {tsv_path}\")\n",
    "            \n",
    "            print(\"\\n--- Data Preview ---\")\n",
    "            print(\"Note: BLEU and ROUGE are corpus-level metrics and are not included in this per-entry file.\")\n",
    "            print(final_results_df.head())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred while creating the TSV file: {e}\")\n",
    "\n",
    "    # --- 4. Clear Memory for Next Model ---\n",
    "    print(f\"\\nClearing memory for model {model_id}...\")\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\\nAll models have been evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce66a70-76be-4b19-8461-f8412cda430e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
