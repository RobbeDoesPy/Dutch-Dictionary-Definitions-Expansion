{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d55ada4-01c6-4274-a145-b47caf522fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch) (69.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.86\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.86:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.86\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.0 requires torch==2.2.0, but you have torch 2.7.1 which is incompatible.\n",
      "torchvision 0.17.0 requires torch==2.2.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 torch-2.7.1 triton-3.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79dd5e1-4d16-41e8-a754-8fb678fb72c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.54.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.46.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch) (69.0.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.7.14)\n",
      "Downloading torchvision-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.0\n",
      "    Uninstalling torchvision-0.17.0:\n",
      "      Successfully uninstalled torchvision-0.17.0\n",
      "Successfully installed torchvision-0.22.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers torch torchvision accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2071081-1b56-42c4-ab81-8e875931bb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrobbe-meersman\u001b[0m (\u001b[33mrobbe-meersman-ku-leuven\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. SETUP AND CONFIGURATION\n",
    "# =============================================================================\n",
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "import wandb\n",
    "\n",
    "# Enable tf32 for better performance on Ampere GPUs\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Configuration settings for the fine-tuning job.\n",
    "    \"\"\"\n",
    "    # Model and dataset configuration\n",
    "    MODEL_ID = \"google/mt5-xl\"\n",
    "    DATASET_NAME = \"RobbedoesHF/dutch-definitions\"\n",
    "    DATASET_TRAIN_SPLIT = \"train\"\n",
    "    DATASET_EVAL_SPLIT = \"validation\"\n",
    "\n",
    "    # LoRA configuration\n",
    "    LORA_R = 32\n",
    "    LORA_ALPHA = 64\n",
    "    LORA_DROPOUT = 0.1\n",
    "    # Target modules for mT5 models\n",
    "    LORA_TARGET_MODULES = [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"] # Common for T5 architectures\n",
    "\n",
    "    # Training configuration\n",
    "    OUTPUT_DIR = \"./results_mt5_xl\"\n",
    "    BATCH_SIZE = 2  # Adjusted for mT5-XL on A100\n",
    "    GRAD_ACCUMULATION_STEPS = 8  # Effective batch size = 16\n",
    "    LEARNING_RATE = 5e-5 # A common starting point for T5 fine-tuning\n",
    "    NUM_TRAIN_EPOCHS = 5\n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    LOGGING_STEPS = 10\n",
    "    SAVE_STEPS = 100\n",
    "    EVAL_STEPS = 100\n",
    "    OPTIMIZER = \"paged_adamw_8bit\"\n",
    "    LR_SCHEDULER_TYPE = \"cosine\"\n",
    "    WARMUP_RATIO = 0.1\n",
    "    EARLY_STOPPING_PATIENCE = 5\n",
    "    SEED = 42\n",
    "\n",
    "    # W&B configuration\n",
    "    WANDB_PROJECT = \"dutch-definition-modeling\"\n",
    "    WANDB_RUN_NAME = \"mt5-xl-qlora-dutch-definitions\"\n",
    "\n",
    "    # Test run configuration\n",
    "    IS_TEST_RUN = False\n",
    "    TEST_RUN_SIZE = 100\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "# --- Weights & Biases Login ---\n",
    "try:\n",
    "    wandb.login()\n",
    "    os.environ[\"WANDB_PROJECT\"] = config.WANDB_PROJECT\n",
    "except Exception as e:\n",
    "    print(f\"Could not log in to W&B: {e}. Disabling monitoring.\")\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b146dbe9-4081-4d4d-91fb-a09fcc2c401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: google/mt5-xl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a920dc87dc9f44efaa21ba21a4c320f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91722364e034cf8a9e31541f9747a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f98508fbe543eca7b84a75b2ce05aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb9c8906e774697af168141326feebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. MODEL AND TOKENIZER LOADING\n",
    "# =============================================================================\n",
    "print(f\"Loading base model: {config.MODEL_ID}\")\n",
    "\n",
    "# Configure quantization for 4-bit loading (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for A100\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the Seq2Seq LM model with quantization\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    config.MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5112e715-12b3-49ca-b995-b39e8520ef64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering datasets for invalid rows...\n",
      "Creating prompts and tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738a2a0e545e4cfa8bf9fc9a1803d9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/27880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f45f7a29db048a0ac525395f1b065b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3494 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Final training dataset size: 27880\n",
      "Final evaluation dataset size: 3494\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. DATASET PREPARATION (REVISED)\n",
    "# =============================================================================\n",
    "\n",
    "def is_valid_example(example):\n",
    "    \"\"\"\n",
    "    Checks if an example row has all the required non-empty fields.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        example.get(\"Lemma\") and isinstance(example[\"Lemma\"], str) and len(example[\"Lemma\"].strip()) > 0 and\n",
    "        example.get(\"DefinitionShort\") and isinstance(example[\"DefinitionShort\"], str) and len(example[\"DefinitionShort\"].strip()) > 0 and\n",
    "        example.get(\"DefinitionFull\") and isinstance(example[\"DefinitionFull\"], str) and len(example[\"DefinitionFull\"].strip()) > 0\n",
    "    )\n",
    "\n",
    "def create_prompts_for_mt5(examples):\n",
    "    \"\"\"\n",
    "    Creates source and target text prompts from a batch of examples.\n",
    "    The source text is now only the direct instruction.\n",
    "    \"\"\"\n",
    "    source_texts = []\n",
    "    target_texts = []\n",
    "\n",
    "    for i in range(len(examples['Lemma'])):\n",
    "        lemma = examples['Lemma'][i]\n",
    "        definition_short = examples['DefinitionShort'][i]\n",
    "        definition_full = examples['DefinitionFull'][i]\n",
    "\n",
    "        # The source text is now only the direct user instruction.\n",
    "        source_text = f\"Breid de volgende korte definitie voor het woord '{lemma}' uit tot een volledige definitie: '{definition_short}'\"\n",
    "        source_texts.append(source_text)\n",
    "\n",
    "        # The target text remains the desired full definition for the decoder.\n",
    "        target_texts.append(definition_full)\n",
    "\n",
    "    return {\"source_text\": source_texts, \"target_text\": target_texts}\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the source and target texts for the model.\n",
    "    \"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source_text\"],\n",
    "        max_length=config.MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"target_text\"],\n",
    "        max_length=config.MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# --- Load and Clean the Dataset ---\n",
    "dataset = load_dataset(config.DATASET_NAME)\n",
    "\n",
    "print(\"Filtering datasets for invalid rows...\")\n",
    "train_dataset = dataset[config.DATASET_TRAIN_SPLIT].filter(is_valid_example, num_proc=4)\n",
    "eval_dataset = dataset[config.DATASET_EVAL_SPLIT].filter(is_valid_example, num_proc=4)\n",
    "\n",
    "# --- Create Prompts and Tokenize ---\n",
    "print(\"Creating prompts and tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(create_prompts_for_mt5, batched=True, num_proc=4, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(create_prompts_for_mt5, batched=True, num_proc=4, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"source_text\", \"target_text\"])\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"source_text\", \"target_text\"])\n",
    "\n",
    "if config.IS_TEST_RUN:\n",
    "    train_dataset = train_dataset.select(range(config.TEST_RUN_SIZE))\n",
    "    eval_dataset = eval_dataset.select(range(config.TEST_RUN_SIZE))\n",
    "\n",
    "# --- Print Final Dataset Sizes ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Final training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Final evaluation dataset size: {len(eval_dataset)}\")\n",
    "print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe8574e6-113f-409b-8207-8d73bbe411f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA and PEFT model...\n",
      "trainable params: 48,758,784 || all params: 3,791,378,432 || trainable%: 1.2860\n",
      "Setting up training arguments...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mprint_trainable_parameters()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting up training arguments...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m training_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGRAD_ACCUMULATION_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOPTIMIZER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUM_TRAIN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use bfloat16 for A100 performance\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLR_SCHEDULER_TYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWARMUP_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLOGGING_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAVE_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEVAL_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwandb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWANDB_RUN_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_by_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Data collator for sequence-to-sequence tasks\u001b[39;00m\n\u001b[1;32m     49\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(\n\u001b[1;32m     50\u001b[0m     tokenizer,\n\u001b[1;32m     51\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     55\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. TRAINER SETUP\n",
    "# =============================================================================\n",
    "print(\"Configuring LoRA and PEFT model...\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=config.LORA_R,\n",
    "    lora_alpha=config.LORA_ALPHA,\n",
    "    lora_dropout=config.LORA_DROPOUT,\n",
    "    target_modules=config.LORA_TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, # Specify task type for encoder-decoder\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"Setting up training arguments...\")\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "    gradient_accumulation_steps=config.GRAD_ACCUMULATION_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=config.OPTIMIZER,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    num_train_epochs=config.NUM_TRAIN_EPOCHS,\n",
    "    fp16=False,\n",
    "    bf16=True, # Use bfloat16 for A100 performance\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type=config.LR_SCHEDULER_TYPE,\n",
    "    warmup_ratio=config.WARMUP_RATIO,\n",
    "    logging_dir=f\"{config.OUTPUT_DIR}/logs\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=config.LOGGING_STEPS,\n",
    "    save_steps=config.SAVE_STEPS,\n",
    "    eval_steps=config.EVAL_STEPS,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=config.WANDB_RUN_NAME,\n",
    "    seed=config.SEED,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "# Data collator for sequence-to-sequence tasks\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=tokenizer.pad_token_id,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=config.EARLY_STOPPING_PATIENCE)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c7540-2bd9-4d19-9299-30de85176364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nSaving final LoRA adapter...\")\n",
    "final_model_path = os.path.join(config.OUTPUT_DIR, \"final_checkpoint\")\n",
    "trainer.save_model(final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
